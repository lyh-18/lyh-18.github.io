<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> news | Yihao LIU </title> <meta name="author" content="Yihao LIU"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://lyh-18.github.io/news/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Yihao</span> LIU </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">news</h1> <p class="post-description"></p> </header> <article> <div class="news"> <div class="table-responsive"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">Jan 27, 2026</th> <td> Four papers accepted by <strong>ICLR’26</strong>. </td> </tr> <tr> <th scope="row" style="width: 20%">Dec 30, 2025</th> <td> I’m happy to share our new work <strong>UniPercept</strong>, which tackles a key blind spot of today’s multimodal LLMs: <em>perceptual-level image understanding</em> — how images look and feel to humans — covering <strong>aesthetics</strong>, <strong>quality</strong>, <strong>structure</strong>, and <strong>texture</strong>. Our release includes <strong>UniPercept-Bench</strong>, a unified benchmark spanning <strong><em>IAA/IQA/ISTA</em></strong> and supporting both Visual Rating (VR) and Visual Question Answering (VQA) evaluations. We also introduce the <strong>UniPercept</strong> baseline model to generalize across VR and VQA settings. Beyond benchmarking, UniPercept can be used as a reward model for post-training text-to-image systems and as a perceptual diagnostic tool for analyzing model outputs and datasets. <a href="https://thunderbolt215.github.io/Unipercept-project/" rel="external nofollow noopener" target="_blank">[Homepage]</a> <a href="https://github.com/thunderbolt215/UniPercept" rel="external nofollow noopener" target="_blank">[GitHub]</a> <a href="https://huggingface.co/datasets/Thunderbolt215215/UniPercept-Bench" rel="external nofollow noopener" target="_blank">[ UniPercept-Bench]</a> <a href="https://huggingface.co/Thunderbolt215215/UniPercept" rel="external nofollow noopener" target="_blank">[ UniPercept Model]</a> <a href="https://arxiv.org/abs/2512.21675" rel="external nofollow noopener" target="_blank">[Paper]</a>. </td> </tr> <tr> <th scope="row" style="width: 20%">Oct 21, 2025</th> <td> We present <strong>PICABench</strong>, a new benchmark and evaluation protocol for assessing <em>physical realism</em> in image editing — an often overlooked dimension in current generative models. PICABench systematically evaluates the physical consequences across eight sub-dimensions spanning optics, mechanics, and state transitions, with a reliable PICAEval protocol combining VLM-as-a-judge and region-level human annotations. We also build <strong>PICA-100K</strong>, a dataset for learning physics from videos. Evaluations show that physical realism remains a major challenge. PICABench aims to drive the next wave of physics-aware, causally consistent image editing. <a href="https://picabench.github.io/" rel="external nofollow noopener" target="_blank">[Homepage]</a> <a href="https://github.com/Andrew0613/PICABench" rel="external nofollow noopener" target="_blank">[GitHub]</a> <a href="https://huggingface.co/datasets/Andrew613/PICABench" rel="external nofollow noopener" target="_blank">[ PICABench Dataset]</a> <a href="https://huggingface.co/datasets/Andrew613/PICA-100K" rel="external nofollow noopener" target="_blank">[ PICA-100K Dataset]</a> <a href="https://arxiv.org/abs/2510.17681" rel="external nofollow noopener" target="_blank">[Paper]</a>. </td> </tr> <tr> <th scope="row" style="width: 20%">Sep 10, 2025</th> <td> We are excited to announce <strong>Lumina-DiMOO</strong>, our latest unified multimodal generation and understanding model built upon an advanced discrete diffusion architecture. This framework demonstrates the strong potential of multimodal diffusion large language models (dLLM) to unify diverse tasks within a single, streamlined architecture, while delivering state-of-the-art performance that surpasses many existing unified models. Learn more and explore resources: <a href="https://synbol.github.io/Lumina-DiMOO/" rel="external nofollow noopener" target="_blank">[Homepage]</a> <a href="https://github.com/Alpha-VLLM/Lumina-DiMOO" rel="external nofollow noopener" target="_blank">[GitHub]</a> <a href="https://huggingface.co/Alpha-VLLM/Lumina-DiMOO" rel="external nofollow noopener" target="_blank">[HuggingFace]</a>. </td> </tr> <tr> <th scope="row" style="width: 20%">Sep 01, 2025</th> <td> We introduce <strong>ArtiMuse</strong>, a multimodal large language model (MLLM) for professional aesthetic understanding, which is trained on <strong>ArtiMuse-10K</strong>, a meticulously curated, expert-annotated dataset. ArtiMuse-10K systematically defines eight explainable and fine-grained aesthetic attributes (e.g., Composition &amp; Design, Visual Elements &amp; Structure), with a wide coverage of diverse visual domains, including graphic design, 3D design, AIGC-generated images, photography, and painting &amp; calligraphy. <a href="https://arxiv.org/abs/2507.14533" rel="external nofollow noopener" target="_blank">[Paper]</a> <a href="https://thunderbolt215.github.io/ArtiMuse-project/" rel="external nofollow noopener" target="_blank">[Homepage]</a> <a href="https://github.com/thunderbolt215/ArtiMuse" rel="external nofollow noopener" target="_blank">[GitHub]</a> <a href="https://artimuse.intern-ai.org.cn/" rel="external nofollow noopener" target="_blank">[Online Demo v1.0]</a> Note: ArtiMuse was officially released at <strong>WAIC 2025</strong>, in the forum “Evolving with AI: The Iteration and Resilience of Artistic Creativity”. </td> </tr> <tr> <th scope="row" style="width: 20%">Jun 26, 2025</th> <td> Our video restoration method <em>DiffVSR</em> was accepted by <strong>ICCV2025</strong>. <a href="https://arxiv.org/abs/2501.10110" rel="external nofollow noopener" target="_blank">[Paper]</a> <a href="https://xh9998.github.io/DiffVSR-project/" rel="external nofollow noopener" target="_blank">[Homepage]</a> </td> </tr> <tr> <th scope="row" style="width: 20%">Apr 22, 2025</th> <td> Our video colorization method <a href="https://github.com/lyh-18/TCVC-Temporally-Consistent-Video-Colorization" rel="external nofollow noopener" target="_blank">TCVC</a> has won the <strong>CVMJ 2025 Best Paper Honorable Mention Award</strong>. </td> </tr> <tr> <th scope="row" style="width: 20%">Apr 01, 2025</th> <td> We present <strong>Lunima-OmniLV</strong> (abbreviated as OmniLV), a universal multimodal multi-task framework for low-level vision that addresses over 100 sub-tasks across four major categories, including image restoration, image enhancement, weak-semantic dense prediction, and stylization. <a href="https://arxiv.org/abs/2504.04903" rel="external nofollow noopener" target="_blank">[Paper]</a> <a href="https://andrew0613.github.io/OmniLV_page/" rel="external nofollow noopener" target="_blank">[Homepage]</a> </td> </tr> <tr> <th scope="row" style="width: 20%">Jul 18, 2024</th> <td> <em>GenLV</em> was accepted by <strong>ACM MM2024</strong>. GenLV is a successive work of PromptGIP, which further broadens the tasks and improves performance. The paper can be found at <a href="https://dl.acm.org/doi/pdf/10.1145/3664647.3681621" rel="external nofollow noopener" target="_blank">here</a>. </td> </tr> <tr> <th scope="row" style="width: 20%">Jul 01, 2024</th> <td> Two papers were accepted by <strong>ECCV2024</strong>. By analyzing the relationships between image degradations, <a href="https://arxiv.org/abs/2407.12273" rel="external nofollow noopener" target="_blank">GRIDS</a> propose a grouped learning method to deal with multiple-degradation restoration. <a href="https://github.com/Andrew0613/X-Restormer" rel="external nofollow noopener" target="_blank">X-Restormer</a> is a new general image restoration backbone network, which possesses good task generality and achieves competitive performance across a variety of restoration tasks. </td> </tr> <tr> <th scope="row" style="width: 20%">May 02, 2024</th> <td> <em>PromptGIP</em> was accepted by <strong>ICML2024</strong>. PromptGIP is a universal model for general image processing that covers image restoration, image enhancement, image feature extraction tasks, etc. Code is available at <a href="https://github.com/lyh-18/PromptGIP" rel="external nofollow noopener" target="_blank">here</a>. </td> </tr> <tr> <th scope="row" style="width: 20%">Aug 27, 2023</th> <td> One paper was accepted by <strong>TPAMI</strong>. <a href="https://github.com/lyh-18/SRGA" rel="external nofollow noopener" target="_blank">SRGA</a> is the first quantitative indicator for measuring the generalization ability of blind super-resolution deep models. </td> </tr> <tr> <th scope="row" style="width: 20%">Mar 25, 2023</th> <td> Two papers were accepted by <strong>CVPR2023</strong>. <a href="https://github.com/lyh-18/DegAE_DegradationAutoencoder" rel="external nofollow noopener" target="_blank">DegAE</a> is a new pretraining paradigm for low-level vision. <a href="https://github.com/haoyuc/MaskedDenoising" rel="external nofollow noopener" target="_blank">MaskedDenoising</a> adopts masked training to enhance the generalization performance of denoising networks. </td> </tr> <tr> <th scope="row" style="width: 20%">Mar 12, 2023</th> <td> Our video colorization method <em>TCVC</em> was accepted by <strong>CVMJ</strong>. Code is available at <a href="https://github.com/lyh-18/TCVC-Temporally-Consistent-Video-Colorization" rel="external nofollow noopener" target="_blank">here</a>. </td> </tr> <tr> <th scope="row" style="width: 20%">Aug 25, 2022</th> <td> Our survey on <strong>Blind Image Super-Resolution</strong> has been accepted by <strong>TPAMI</strong>. <a href="https://ieeexplore.ieee.org/abstract/document/9870558" rel="external nofollow noopener" target="_blank">Paper Link</a> </td> </tr> <tr> <th scope="row" style="width: 20%">May 30, 2022</th> <td> The extention version of the lightweight photo retouching network <em>CSRNet</em> has been accepted by <strong>TMM</strong>. Paper links: <a href="https://link.springer.com/chapter/10.1007/978-3-030-58601-0_40" rel="external nofollow noopener" target="_blank">conference version</a>, <a href="https://arxiv.org/abs/2104.06279" rel="external nofollow noopener" target="_blank">journal version</a>. </td> </tr> <tr> <th scope="row" style="width: 20%">Aug 31, 2020</th> <td> We have won the <strong>3rd place</strong> of <a href="https://rlq-tod.github.io/challenge2.html" rel="external nofollow noopener" target="_blank">UDC2020 Challenge on Image Restoration of Under-Display Camera</a> (in conjunction with ECCV2020). The technique report of the proposed RDUnet model can be found at <a href="https://link.springer.com/chapter/10.1007/978-3-030-68238-5_30" rel="external nofollow noopener" target="_blank">here</a>. The official challenge report can be found at <a href="https://arxiv.org/pdf/2008.07742" rel="external nofollow noopener" target="_blank">here</a>. </td> </tr> <tr> <th scope="row" style="width: 20%">Aug 26, 2020</th> <td> We have won the <strong>1st place</strong> of <a href="https://data.vision.ee.ethz.ch/cvl/aim20/" rel="external nofollow noopener" target="_blank">AIM2020 Video Temporal Super-Resolution Challenge</a> (in conjunction with ECCV2020). The technique report of the proposed <em>EQVI</em> model can be found at <a href="https://arxiv.org/abs/2009.04642" rel="external nofollow noopener" target="_blank">here</a>. Code is available at <a href="https://github.com/haoyuc/MaskedDenoising" rel="external nofollow noopener" target="_blank">here</a>. The official challenge report can be found at <a href="https://arxiv.org/pdf/2009.12987" rel="external nofollow noopener" target="_blank">here</a>. </td> </tr> <tr> <th scope="row" style="width: 20%">Jul 18, 2020</th> <td> Our proposed lightweight photo retouching method <a href="https://link.springer.com/chapter/10.1007/978-3-030-58601-0_40" rel="external nofollow noopener" target="_blank">CSRNet</a> was accepted by <strong>ECCV2020</strong>. </td> </tr> </table> </div> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Yihao LIU. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?70d799092f862ad98c7876aa47712e20"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>